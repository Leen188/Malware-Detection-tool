from urllib.parse import unquote, urlparse, parse_qs
from pathlib import PurePosixPath
import socket
from urllib.parse import urlparse
import re
import urllib3
import warnings
import tldextract
import requests
import pandas as pd
import joblib
import numpy as np
from bs4 import BeautifulSoup


class URLFeatures:
    def __init__(self, url):
        self.url = url
        self.validate_url()

    def validate_url(self):
        try:
            result = urlparse(self.url)
            if all([result.scheme, result.netloc]):
                return True
            else:
                raise ValueError("Invalid URL")
        except ValueError as e:
            print(e)
            exit(1)

    def Dots(self):
        return self.url.count('.')

    def SubdomainLevel(self):
        ext = tldextract.extract(self.url)
        Subdomain = ext.subdomain
        return sum(map(Subdomain.strip().count, ['.'])) + 1

    def Path_level(self):
        p = urlparse(self.url)
        Path = p.path
        return Path.count('/')

    def UrlLength(self):
        return len(self.url)

    def NumDash(self):
        return self.url.count('-')

    def NumDashInHostname(self):
        ext = tldextract.extract(self.url)
        Domain = ext.domain
        return Domain.count('-')

    def NumUnderscore(self):
        NumUnderscore = self.url.count('_')
        return NumUnderscore

    def NumPercent(self):
       NumPercent = self.url.count('%')
       return NumPercent

    def NumQueryComponents(self):

        parsed_url = urlparse(self.url)
        query_string = parsed_url.query
        query_dict = parse_qs(query_string)
        NumQueryComponents = len(query_dict.keys())
        return NumQueryComponents

    def NumAmpersand(self):
        NumAmpersand = self.url.count('&')
        return NumAmpersand

    def NumNumericChars(self):
        url = self.url
        numreic = re.findall(r'\d', url)
        NumNumericChars = len(numreic)
        return NumNumericChars

    def NoHttps(self):  # verify the concept
        response = requests.get(self.url)
        if response.url.startswith('https'):
           return 0
        else:
           return 1

    def DomainInPaths(self):
        netloc = urlparse(self.url).netloc
        try:
          socket.gethostbyname(netloc)
          return 1
        except:
          return 0

    def HostnameLength(self):
        ext = tldextract.extract(self.url)
        HostnameLength = ext.domain
        return len(HostnameLength)

    def PathLength(self):
        PathLength = urlparse(self.url)
        return len(PathLength.path)

    def QueryLength(self):
        parsed_url = urlparse(self.url)
        query_string = parsed_url.query
        QueryLength = len(query_string)
        return QueryLength

    def NumSensitiveWords(self):
        # Define a list of sensitive words
        sensitive_words = ["secure", "account", "webscr",
                           "login", "ebayisapi", "signin", "banking", "confirm",
                           "invoice", "new", "message", "required",
                           "100% free", "earn extra cash", "financial freedom", "free access",
                           "free gift", "free hosting", "free info", "free investment",
                           "free membership", "free money", "free preview", "free quote",
                           "free trial", "full refund", "get out of debt", "get paid",
                           "giveaway", "guaranteed", "increase sales", "increase traffic",
                           "incredible deal", "lower rates", "lowest price", "make money",
                           "million dollars", "miracle", "money back", "once in a lifetime",
                           "one time", "pennies a day", "potential earnings", "prize",
                           "promise", "pure profit", "risk-free", "satisfaction guaranteed",
                           "save big money", "save up to", "special promotion",
                           "act now", "apply now", "become a member", "call now", "click below", "click here"
                           ]

    # Send a GET request to the website
        response = requests.get(self.url)

    # Extract the HTML content from the response
        html_content = response.content

    # Create a BeautifulSoup object to parse the HTML content
        soup = BeautifulSoup(html_content, 'html.parser')

    # Get the text content of the website
        text_content = soup.get_text()

    # Check if any sensitive word is in the text content
        for word in sensitive_words:
           if word in text_content:
              return -1

        return 1

    def PctExtHyperlinks(self):
        response = requests.get(self.url)
        soup = BeautifulSoup(response.text, 'html.parser')
        external_link = 0
        internal_link = 0

        for link in soup.find_all('a'):
            href = link.get('href')
            if href:
               parsed_url = urlparse(href)
               if parsed_url.netloc and parsed_url.netloc != urlparse(self.url).netloc:
                  external_link += 1
               else:
                  internal_link += 1
        total_link = external_link + internal_link
        if total_link == 0:
            return 0
        else:
            external_percentage = (external_link / total_link) * 100
            return external_percentage

    def PctExtResourceUrls(self):
        response = requests.get(self.url)
        soup = BeautifulSoup(response.content, 'html.parser')
        external_res = 0
        internal_res = 0
        link = soup.find_all(['link', 'img', 'script'])
        for resource in link:
            src = resource.get('src')
            if src:
               parsed_url = urlparse(src)
               if parsed_url.netloc and parsed_url.netloc != urlparse(self.url).netloc:
                  external_res += 1
               else:
                  internal_res += 1
        total_res = external_res + internal_res
        if total_res == 0:
            return 0
        else:
            external_percentage = (external_res / total_res) * 100
            return external_percentage

    def ExtFavicon(self):
        page = requests.get(self.url)
        soup = BeautifulSoup(page.content, 'html.parser')
        icon_link = soup.find('link', rel='shortcut icon')
        if icon_link is None:
           icon_link = soup.find('link', rel='icon')
        if icon_link is None:
           return 0
        return 1

    def InsecureForms(self):
        with warnings.catch_warnings(record=True) as w:
            urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
            response = requests.get(self.url)

        if any(issubclass(w.category, urllib3.exceptions.InsecureRequestWarning) for w in w) or response.url.startswith('http://'):
           return 1  # There is an InsecureRequestWarning.
        else:
            return  # There is no InsecureRequestWarning.

    def ExtFormAction(self):
        # Send a GET request to the website
        response = requests.get(self.url)

        # Extract the HTML content from the response
        html_content = response.content

        # Create a BeautifulSoup object to parse the HTML content
        soup = BeautifulSoup(html_content, 'html.parser')

        # Find all forms in the HTML content
        form_tags = soup.find_all('form')

        # Calculate the total number of forms and external forms in the HTML content
        total_forms = len(form_tags)
        external_forms = 0
        for form in form_tags:
            if 'action' in form.attrs and 'http' in form['action']:
                external_forms += 1

        # Calculate the ExtFormAction feature
        if total_forms == 0:
            return 0  # or whatever value makes sense in your case
        else:
            calculate_ExtFormAction = external_forms / total_forms
            return calculate_ExtFormAction

    def PctNullSelfRedirectHyperlinks(self):
        # Send a GET request to the website
        response = requests.get(self.url)

        # Extract the HTML content from the response
        html_content = response.content

        # Create a BeautifulSoup object to parse the HTML content
        soup = BeautifulSoup(html_content, 'html.parser')

        # Find all external links in the HTML content
        external_links = []
        for link in soup.find_all('a'):
            if link.has_attr('href') and 'http' in link['href']:
                external_links.append(link['href'])

        # Calculate the total number of external links and external links with null or self redirect hyperlinks
        total_links = len(external_links)
        null_self_redirect_links = 0
        for link in external_links:
            response = requests.get(link)
            if response.url == link:
                null_self_redirect_links += 1

        # Calculate the PctNullSelfRedirectHyperlinks feature
        if total_links == 0:
            return 0  # or whatever value makes sense in your case
        else:
            calculate_PctNullSelfRedirectHyperlinks = null_self_redirect_links / total_links
            return calculate_PctNullSelfRedirectHyperlinks

    def FrequentDomainNameMismatch(self):
        # Send a GET request to the website
        response = requests.get(self.url)

        # Extract the HTML content from the response
        html_content = response.content

        # Create a BeautifulSoup object to parse the HTML content
        soup = BeautifulSoup(html_content, 'html.parser')

        # Find all links in the HTML content
        links = []
        for link in soup.find_all('a'):
            if link.has_attr('href') and 'http' in link['href']:
                links.append(link['href'])

        # Calculate the most frequent domain name in the HTML source code
        domain_counts = {}
        for link in links:
            domain = urlparse(link).netloc
            if domain in domain_counts:
                domain_counts[domain] += 1
            else:
                domain_counts[domain] = 1

        if not domain_counts:  # Check if domain_counts is empty
            return 0  # or whatever value makes sense in your case

        most_frequent_domain = max(domain_counts, key=domain_counts.get)

        # Calculate the FrequentDomainNameMismatch feature
        domain_name = urlparse(self.url).netloc
        if domain_name == most_frequent_domain:
            calculate_FrequentDomainNameMismatch = 0
        else:
            calculate_FrequentDomainNameMismatch = 1

        return calculate_FrequentDomainNameMismatch

    def SubmitInfoToEmail(self):
        # Make a request to the website
        r = requests.get(self.url)
        r.content

    # Use the 'html.parser' to parse the page
        soup = BeautifulSoup(r.content, 'html.parser')

        # Find potential submit buttons
        potential_buttons = soup.find_all(['input', 'button'])

        # Check each button for the specified value or text
        has_button = any(button.get('value', '').lower() == 'submit info to email' or button.text.lower(
        ) == 'submit info to email' for button in potential_buttons)

        # Print whether the website has the specified button
        if has_button:
            return 1
        else:
            return 0

    def IframeOrFrame(self):
        # Make a request to the website
        r = requests.get(self.url)
        r.content

        # Use the 'html.parser' to parse the page
        soup = BeautifulSoup(r.content, 'html.parser')

        # Find potential iframes or frames
        potential_iframes = soup.find_all(['iframe', 'frame'])

        # Print whether the website has an iframe or frame
        if potential_iframes:
            return 1
        else:
            return 0

    def MissingTitle(self):
        # Make a request to the website
        r = requests.get(self.url)
        r.content

    # Use the 'html.parser' to parse the page
        soup = BeautifulSoup(r.content, 'html.parser')

    # Find the title tag
        title_tag = soup.title

    # Print whether the website has a title
        if title_tag:
          return 0
        else:
          return 1

    def AbnormalExtFormActionR(self):
        # Send a GET request to the website
        response = requests.get(self.url)

        # Parse the HTML content using BeautifulSoup
        soup = BeautifulSoup(response.content, "html.parser")

        # Check each form for an external action URL
        for form in soup.find_all("form"):
            if form.has_attr("action"):
                parsed_url = urlparse(form["action"])
                if parsed_url.netloc != urlparse(url).netloc:
                    return 1

        # Return 0 if no abnormal form action was found
        return 0

    def AbnormalExtMetaScriptLinkRT(self):
        # Send a GET request to the website
        response = requests.get(self.url)

        # Parse the HTML content using BeautifulSoup
        soup = BeautifulSoup(response.content, "html.parser")

        # Check each meta and script tag for an external URL
        for tag in soup.find_all(['meta', 'script']):
            if tag.has_attr("src") or tag.has_attr("href"):
                parsed_url = urlparse(tag.get("src", tag.get("href")))
                if parsed_url.netloc and parsed_url.netloc != urlparse(self.url).netloc:
                    return -1

        # Return -1 if no abnormal meta or script tag was found
        return 1

    def PctExtNullSelfRedirectHyperlinksRT(self):
        # Send a GET request to the website
        response = requests.get(self.url)

        # Extract the HTML content from the response
        html_content = response.content

        # Create a BeautifulSoup object to parse the HTML content
        soup = BeautifulSoup(html_content, 'html.parser')

        # Find all external links in the HTML content
        external_links = []
        for link in soup.find_all('a'):
            if link.has_attr('href') and 'http' in link['href']:
                external_links.append(link['href'])

        # Calculate the total number of external links and external links with null or self redirect hyperlinks
        total_links = len(external_links)
        null_self_redirect_links = 0
        for link in external_links:
            response = requests.get(link)
            if response.url == link:
                null_self_redirect_links += 1

        # Calculate the PctExtNullSelfRedirectHyperlinksRT feature
        if total_links == 0:
            return 0  # or whatever value makes sense in your case
        else:
            PctExtNullSelfRedirectHyperlinksRT = null_self_redirect_links / total_links
            return PctExtNullSelfRedirectHyperlinksRT
    
    # def make_prediction(self):
    #     # Load the model
    #     xgb_model = joblib.load('base/ML/URL_ML/xgb_model.joblib')

    #     # Your URL analysis code
    #     methods = {
    #         'Dots': self.Dots(),
    #         'SubdomainLevel': self.SubdomainLevel(),
    #         'Path_level': self.Path_level(),
    #         'UrlLength': self.UrlLength(),
    #         'NumDash': self.NumDash(),
    #         'NumDashInHostname': self.NumDashInHostname(),
    #         'NumUnderscore': self.NumUnderscore(),
    #         'NumPercent': self.NumPercent(),
    #         'NumQueryComponents': self.NumQueryComponents(),
    #         'NumAmpersand': self.NumAmpersand(),
    #         'NumNumericChars': self.NumNumericChars(),
    #         'NoHttps': self.NoHttps(),
    #         'DomainInPaths': self.DomainInPaths(),
    #         'HostnameLength': self.HostnameLength(),
    #         'PathLength': self.PathLength(),
    #         'QueryLength': self.QueryLength(),
    #         'NumSensitiveWords': self.NumSensitiveWords(),
    #         'PctExtHyperlinks': self.PctExtHyperlinks(),
    #         'PctExtResourceUrls': self.PctExtResourceUrls(),
    #         'ExtFavicon': self.ExtFavicon(),
    #         'InsecureForms': self.InsecureForms(),
    #         'ExtFormAction': self.ExtFormAction(),
    #         'PctNullSelfRedirectHyperlinks': self.PctNullSelfRedirectHyperlinks(),
    #         'FrequentDomainNameMismatch': self.FrequentDomainNameMismatch(),
    #         'SubmitInfoToEmail': self.SubmitInfoToEmail(),
    #         'IframeOrFrame': self.IframeOrFrame(),
    #         'MissingTitle': self.MissingTitle(),
    #         'AbnormalExtFormActionR': self.AbnormalExtFormActionR(),
    #         'AbnormalExtMetaScriptLinkRT': self.AbnormalExtMetaScriptLinkRT(),
    #         'PctExtNullSelfRedirectHyperlinksRT': self.PctExtNullSelfRedirectHyperlinksRT(),
    #     }

    #     # Convert the dictionary values to a list
    #     features = list(methods.values())

    #     # Reshape the features to a 2D array
    #     features = np.array(features).reshape(1, -1)

    #     # Make the prediction
    #     predictions = xgb_model.predict(features)

    #     # Check the prediction result
    #     if predictions[0] == 'no':
    #         return  "The URL is safe"
    #     elif predictions[0] == 'yes':
    #         return "The URL is malicious"


    #     # Your DataFrame code
    #     df = pd.DataFrame([methods])
    #     df.to_csv('results.csv', index=False)

       


